# -*- coding: utf-8 -*-
"""Chat_Analyzer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kMB9TcMBsLAaC6GLrl8lkYtSnNRmRE3c

"""
# **Change PATH depending on your enviroment**
chat_file_path  = './ChatCollector/chat.log' # set chat_log path what you want parse E.g chat_file_path='./twitch_chat_log/chat_paka.log'
original_excel_file_path = './ChatCollector/Chat_Analyzed/original.xlsx' # set SAVE path from parsed chat_log E.g. excel_file_path = './유승민/Dataset_Twitch_Chat.xlsx'
subword_excel_file_path = './ChatCollector/Chat_Analyzed/subword.xlsx' # set SAVE path from parsed chat_log that include subword group E.g. excel_file_path = './유승민/Dataset_Twitch_Chat.xlsx'
rareword_excel_file_path = './ChatCollector/Chat_Analyzed/rareword.xlsx' # set SAVE path from parsed chat_log that include rareword group
table = ['datetime', 'channel', 'username', 'message', 'type']
subword_iterated = [
    # 'ㅋ',
    # '캬'
    ]
top_k=40

"""---
---
---
---
---
---
---
---
---
---
---
---
---
---
---
---
---
---
---
---
---
---
---

# *Don't fix under code*
"""

import nltk
import pandas as pd
import re
import torch
import json
from nltk import FreqDist
from nltk.tokenize import word_tokenize
from datetime import datetime
from torch.utils.data import DataLoader, TensorDataset
from transformers import BertModel, DistilBertModel, AutoTokenizer
from sklearn.model_selection import train_test_split
nltk.download('punkt')
nltk.download('stopwords')

def get_chat_dataframe(file):
    data = []

    with open(file, 'r', encoding='utf-8') as f:
        lines = f.read().split('\n\n\n')

        for line in lines:
            try:
                time_logged = line.split('—')[0].strip()
                time_logged = datetime.strptime(time_logged, '%Y-%m-%d_%H:%M:%S')

                username_message = line.split('—')[1:]
                username_message = '—'.join(username_message).strip()

                username, channel, message = re.search(
                    ':(.*)\\!.*@.*\\.tmi\\.twitch\\.tv PRIVMSG #(.*) :(.*)', username_message
                ).groups()

                d = {
                    table[0] : time_logged,
                    table[1] : channel,
                    table[2] : username,
                    table[3] : message,
                    table[4] : type,
                }

                data.append(d)

            except Exception:
                pass

    return pd.DataFrame().from_records(data)

df = get_chat_dataframe(chat_file_path)
# Specify the Excel file path
df.to_excel(original_excel_file_path, index=False)

df = pd.read_excel(original_excel_file_path)

print(df)

# Combine all messages into a single string
all_messages = ' '.join(df['message'].astype(str))

# Tokenize the string into words
words = all_messages.split()

# Create a DataFrame for word count
df_word_count = pd.DataFrame(pd.Series(words).value_counts().reset_index())
df_word_count.columns = ['word', 'count']

# Sort the DataFrame by count in descending order
df_word_count = df_word_count.sort_values(by='count', ascending=False).reset_index(drop=True)
top_k_words = df_word_count.head(top_k)['word'].tolist()

print("Top {} words:".format(top_k))
print(top_k_words)

df2 = pd.DataFrame(columns=table)
df3 = pd.DataFrame(columns=table)
row_to_move = []
row_to_move2 = []
for i in subword_iterated:
  row_to_move.append(df[df['message'].str.count(i) > 0])
#   row_to_move.concat(df[df['message'].str.count(i) > 0])
for i in top_k_words:
  row_to_move.append(df[df['message'] == i])
#   row_to_move.concat(df[df['message'] == i])
df2 = df2.append(row_to_move, ignore_index=True)

merged_df = pd.merge(df, df2, how='outer', indicator=True).loc[lambda x: x['_merge'] == 'left_only']
df3 = merged_df.drop(columns=['_merge']).reset_index(drop=True)

df2.to_excel(subword_excel_file_path,index=False)
df3.to_excel(rareword_excel_file_path,index=False)

"""# ---
---
---
---
---
---
---
---
---
---
---
---
---
---
---
---
---
---
---
---
---
---
---
---

# # **실험실**
# """

# # # Combine all messages into a single string
# # all_messages = ' '.join(df['message'].astype(str))

# # # Tokenize the string into words
# # words = all_messages.split()

# # # Create a DataFrame for word count
# # df_word_count = pd.DataFrame(pd.Series(words).value_counts().reset_index())
# # df_word_count.columns = ['word', 'count']

# # # Sort the DataFrame by count in descending order
# # df_word_count = df_word_count.sort_values(by='count', ascending=False).reset_index(drop=True)
# # k=40
# # top_k_words = df_word_count.head(k)['word'].tolist()
# # selected_words = df_word_count[df_word_count['count'] < 10]['word'].tolist()
# # print("Top {} words:".format(k))
# # print(selected_words)
# # sel = pd.DataFrame(columns=table)
# # sel = sel.append(selected_words, ignore_index=True)
# # sel.to_excel(rareword_excel_file_path,index=False)

# # GPU를 사용할 경우
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# # Load pre-trained KoBERT model and tokenizer
# model = BertModel.from_pretrained('monologg/kobert')
# tokenizer = AutoTokenizer.from_pretrained("monologg/kobert")

# # Sample sentences in Korean
# sentence1 = '파카님 크샨테점수 빼면 저친구가 더 높지 않을까요'
# sentence2 = "파카님 크샨테한 거 점수 깎이면 저 친구보다 더 낮으신거 아니에요?"

# # Tokenize and encode the sentences
# inputs1 = tokenizer(sentence1, return_tensors="pt")
# inputs2 = tokenizer(sentence2, return_tensors="pt")

# # Get the model outputs
# outputs1 = model(**inputs1)
# outputs2 = model(**inputs2)

# # Extract the [CLS] token embeddings
# embeddings1 = outputs1.pooler_output
# embeddings2 = outputs2.pooler_output

# # Calculate cosine similarity between the embeddings
# cosine_similarity = torch.nn.functional.cosine_similarity(embeddings1, embeddings2).item()

# print("Similarity Score:", cosine_similarity)

# # Sample DataFrame
# df = pd.DataFrame({
#     'datetime': ['2023-01-01', '2023-01-02', '2023-01-03'],
#     'channel': ['Channel1', 'Channel2', 'Channel1'],
#     'username': ['User1', 'User2', 'User3'],
#     'message': ['ㅋㅋㅋㅋ', '한글 메시지', '반갑습니다']
# })

# # Function to check if a word is composed of initial consonants
# def is_initial_consonant_only(word):
#     # Check if the word is composed of Hangul Jamo initial consonants
#     return re.match('^[ㄱ-ㅎ]+$', word) is not None

# # Apply the function to each word in the 'message' column
# df['is_initial_consonant'] = df['message'].apply(lambda x: ' '.join([str(is_initial_consonant_only(word)) for word in x.split()]))

# # Create a new DataFrame df2 with selected columns
# df2 = df[['datetime', 'channel', 'username', 'is_initial_consonant']]

# # Filter df2 to include only rows where all words are composed of initial consonants
# df2 = df2[df2['is_initial_consonant'] == 'True']

# print(df2)

# words = "The quick brown fox jumped over the lazy dog. The lazy dog yawned."


# # Remove stopwords
# stopwords = set(nltk.corpus.stopwords.words('english'))
# filtered_words = [word.lower() for word in words if word.isalpha() and word.lower() not in stopwords]

# # Calculate word frequencies
# freq_dist = FreqDist(filtered_words)

# # Plot the frequency distribution
# freq_dist.plot(30, cumulative=False)
# threshold = 1.0
# rare_words = [word for word, freq in freq_dist.items() if freq < threshold]
# print(filtered_words)